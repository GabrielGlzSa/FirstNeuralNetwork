{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# My first Neural Network in Python\n",
            "\n",
            "\n",
            "In this Lab, you will program your first neural network using python and the numpy library for array operations. In the last exercise, you will use all the functions you implemented to initialize your first neural network and make a forward propagation. In following labs, you will learn how to train a neural network.\n",
            "\n",
            "\n",
            "\n",
            "**IMPORTANT: This notebook will be graded every time you save the notebook. After saving the notebook, wait 1 second before refreshing the window to see the feedback appended at the end of the notebook. Do not modify any of the functions names that will be graded.** \n",
            "\n",
            "## Tasks\n",
            "\n",
            "- Implement the ReLu activation function for a layer.\n",
            "- Implement a softmax layer.\n",
            "- Initialize a 3 layer neural network.\n",
            "- Implement forward propagation through a neural network.\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 1: Implement the ReLu activation function\n",
            "\n",
            "\n",
            "\n",
            "The Rectifier Linear (ReLU) is a function used as an activation function for neurons for its non-linearity. The function is applied per neuron independently. The following figure shows that ReLu returns a 0 for all negative values of x.\n",
            "\n",
            "\n",
            "![ReLu activation function](./images/relu.png)\n",
            "\n",
            "\n",
            "Your first task is to implement the ReLu activation function for a single value. Then, you will follow the same strategy for an array of values, which represent the outputs of $n$ neurons for $k$ samples of data.\n",
            "\n",
            "ReLu exercises:\n",
            "\n",
            "- Exercise1: Implement the relu function for single value.\n",
            "- Exercise2: Implement the relu function for $k$ samples and $n$ neurons.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "\n",
            "# Exercise 1\n",
            "def relu_function(x):\n",
            "    \"\"\"\n",
            "    Implement the ReLu function for a scalar.\n",
            "    HINT: there is a python built-in function that does exactly what the\n",
            "    figure shows when one of the parameters is 0.\n",
            "    x: a float\n",
            "    return: a float\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = max(0,x)\n",
            "    return o\n",
            "\n",
            "\n",
            "\n",
            "output = relu_function(5)\n",
            "print(output) # Expected output: 5\n",
            "output = relu_function(-2)\n",
            "print(output) # Expected output: 0"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that you understand that the ReLu function is really a max function, you have to implement it for a layer. The strategy is the same as before: perform a max operation between of each element in an array and 0. There are multiple ways to achieve it. Although you could use a loop, there is function in the *numpy* library that can produce the expected result in a single line of code."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 2\n",
            "def relu_function_layer(x):\n",
            "    \"\"\"\n",
            "    Implement the ReLu function for an array.\n",
            "    HINT: there are multiple ways to achieve it using numpy.\n",
            "    x: a numpy array of floats\n",
            "    return: a numpy array of floats\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = np.maximum(x,0)\n",
            "    return o\n",
            "\n",
            "\n",
            "x = np.array([[5, -2, 3, 4, -5], [-1, 2, 3, 4, 5]])\n",
            "output = relu_function_layer(x)\n",
            "print(output) # Expected output: [[5, 0, 3, 4, 0], [0, 2, 3, 4, 5]]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 2: Implement a softmax layer.\n",
            "\n",
            "The softmax layer is a dense layer that uses the softmax function to convert the logits into probabilities. The logits are the raw predictions of the neurons. The prediction of neuron $i$ is calculated using the following equation:\n",
            "\n",
            ">$z_i=\\mathbf{x} \\cdot \\mathbf{w}_i + b_i$,\n",
            "\n",
            "where $\\mathbf{x}$ is the input vector of the layer for a single sample, $\\mathbf{w}_i$ is the weights of the neuron $i$ and $b_i$ is a scalar known as the bias. In contrast to the ReLu activation function that is applied to each neuron independently, the softmax function takes into consideration the outputs of all neurons in a layer to assign a probability to each neuron using the following equation\n",
            "\n",
            ">$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}$,\n",
            "\n",
            "where $z_i$ is the logit of neuron $i$, $z_j$ is the logit of neuron $j$ and $e$ is euler's number. Since the softmax function converts all logits into probabilities, their sum adds to one: \n",
            "\n",
            "> $\\sum_{j}^{n} \\sigma(z_{j}) = 1$.\n",
            "\n",
            "\n",
            "In the following exercise, you will calculate the output of each neuron independently to understand how a neuron works. Then, you will calculate the logits for all neurons in a layer. Finally, you will use the numpy library to implement the softmax function.\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 3\n",
            "def calculate_neuron_logit(x, w, b):\n",
            "    \"\"\"\n",
            "    Calculate the logit of a neuron for a single sample by multiplying two vectors and summing the bias.\n",
            "    x: input vector\n",
            "    w: weight vector\n",
            "    b: floating-point number bias\n",
            "    return: a floating-point number.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    logit = x@w+b\n",
            "    return logit\n",
            "\n",
            "# Generate a dummy input with 10 features.\n",
            "x = np.random.rand(10)\n",
            "# Randomly initialize the weight for 10 features.\n",
            "w = np.random.rand(10)\n",
            "# Randomly initialize the bias.\n",
            "b = np.random.rand(1)\n",
            "\n",
            "logit = calculate_neuron_logit(x, w, b)\n",
            "print(logit)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that you have learned to calculate the logit of a neuron for a single sample. You can move on to learning how to process multiple samples in parallel. \n",
            "\n",
            "A neural network receives $k$ samples the same time and generates an output per sample. If the samples have $f$ features, the input of the network is really a matrix with $k$ rows and $f$ columns. Since the input is a matrix, we shall refer to it as $\\mathbf{X}$. We can get the output of a neuron by multiplying $\\mathbf{X}$ by its weight vector $\\mathbf{w}$. \n",
            "\n",
            "$z_{i} = \\mathbf{X} \\mathbf{w}_i + b_i$\n",
            "\n",
            "Nonetheless, there is a better approach. Imagine we have a weight matrix $\\mathbf{W}$ that has all the weights of all neurons in a layer. One dimension would be the number of neurons $n$ and the other would be the number of input features $f$ (number of weights in each neuron). We could calculate the output of all neurons for all samples with by multiplying the input matrix $\\mathbf{X}$ by the weight matrix $\\mathbf{W}$:\n",
            "\n",
            "$\\mathbf{Z} = \\mathbf{X} \\mathbf{W} + \\mathbf{b}$\n",
            "\n",
            "Since the multiplication requires that the number of columns in matrix $\\mathbf{X}$ be equal to the number of rows in matrix $\\mathbf{W}$, the matrix $\\mathbf{W}$ must have $f$ rows and $n$ columns. In other words, all weights associated with neuron $i$ will be in column $i$. \n",
            "\n",
            "  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 4\n",
            "def calculate_logits_layer(x, w, b):\n",
            "    \"\"\"\n",
            "    Calculate the logits of all neurons in parallel.\n",
            "    HINT: USE THE MATRIX MULTIPLICATION IMPLEMENTED IN NUMPY.\n",
            "    x: input matrix\n",
            "    w: weight matrix\n",
            "    b: bias vector\n",
            "    return: a numpy array of logits.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = x@w+b\n",
            "    return o\n",
            "\n",
            "# Generate a dummy input with 3 samples and 8 features.\n",
            "X = np.random.uniform(low=-1.0, high=1.0, size=(3,8))\n",
            "# Randomly initialize the weight for 8 features and 4 neurons.\n",
            "W = np.random.uniform(low=-1.0, high=1.0, size=(8, 4))\n",
            "# Randomly initialize the bias for 4 neurons.\n",
            "B = np.random.uniform(low=-1.0, high=1.0, size=4)\n",
            "\n",
            "logits = calculate_logits_layer(X, W, B)\n",
            "print(logits.shape) # Expected shape is (3, 4)\n",
            "print(logits)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 5\n",
            "def softmax_layer(x, w, b):\n",
            "    \"\"\"\n",
            "    Implement a softmax layer by calculating the logits of a layer\n",
            "    and applying the softmax function to the logits.\n",
            "    HINT: YOU CAN USE exp() AVAILABLE IN NUMPY FOR IMPLEMENTING SOFTMAX.\n",
            "    x: input matrix\n",
            "    w: weight matrix\n",
            "    b: bias vector\n",
            "    \"\"\"\n",
            "    logits = calculate_logits_layer(x, w, b)\n",
            "\n",
            "    probabilities =  np.exp(logits) / np.sum(np.exp(logits), axis=1).reshape(-1,1) # Your code here\n",
            "    \n",
            "    \n",
            "    return probabilities\n",
            "\n",
            "\n",
            "probabilities = softmax_layer(X, W, B)\n",
            "print(probabilities)\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 3: Implement a neural network\n",
            "\n",
            "For this task, you will create a neural network using the functions you have implemented so far. The neural network must have the following architecture:\n",
            "\n",
            "1. Dense layer with **ReLu activation function** with **32 neurons**.\n",
            "2. Dense layer with **Relu activation function** with **32 neurons**.\n",
            "3. Dense layer with **softmax activation function** with **4 neurons**.\n",
            "\n",
            "The weights of the neural network should be randomly initialized. You can use  the normal distribution as shown in the next function. **Take into consideration that the number of features per sample is 8**."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 6\n",
            "def weight_initialization(f, n1, n2, n3):\n",
            "    \"\"\"\n",
            "    Randomly initialize the weights of a neural network \n",
            "    with 3 layers using a normal distribution.\n",
            "    HINT: Use numpy to generate random matrices.\n",
            "    f: number of features\n",
            "    n1: number of neurons in the first hidden layer\n",
            "    n2: number of neurons in the second hidden layer\n",
            "    n3: number of neurons in the output layer\n",
            "    return: a tuple of weight matrices\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    w1 = np.random.normal(size=(f,n1)) #Replace contants with the corresponding variables.\n",
            "    w2 = np.random.normal(size=(n1,n2))\n",
            "    w3 = np.random.normal(size=(n2,n3))\n",
            "    b1 = np.random.normal(size=(n1))\n",
            "    b2 = np.random.normal(size=(n2))\n",
            "    b3 = np.random.normal(size=(n3))\n",
            "    return w1, w2, w3, b1, b2, b3\n",
            "\n",
            "# Example of weight initialization for layers with 10, 8, 6 and 4 neurons, respectively.\n",
            "w1, w2, w3, b1, b2, b3 = weight_initialization(10, 8, 6, 4)\n",
            "print(w1.shape) # Expected output: (10, 8)\n",
            "print(w2.shape) # Expected output: (8, 6)\n",
            "print(w3.shape) # Expected output: (6, 4)\n",
            "print(b1.shape) # Expected output: (8,)\n",
            "print(b2.shape) # Expected output: (6,)\n",
            "print(b3.shape) # Expected output: (4,)\n",
            "\n",
            "# Modify the parameters to create the specified neural network architecture.\n",
            "w1, w2, w3, b1, b2, b3 = weight_initialization(10, 8, 6, 4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The process of calculating the output of a neural network is called by several names like fordward propagation and inference. In the following exercise, you will use the functions you implemented for forward propagation of the specified network architecture. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def neural_network(x, w1, b1, w2, b2, w3, b3):\n",
            "    \"\"\"\n",
            "    Generate the output for a neural network with 3 layers.\n",
            "    x: input matrix\n",
            "    wn: weight matrix of layer n.\n",
            "    bn: bias vector of layer n.\n",
            "    return: tuple of numpy arrays for output of first layer, output of second layer, output of the network.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "\n",
            "    o1 = relu_function_layer(calculate_logits_layer(x,w1,b1)) # Output of first layer.\n",
            "    o2 = relu_function_layer(calculate_logits_layer(o1,w2,b2)) # Output of second layer.\n",
            "    probabilities = softmax_layer(o2, w3,b3) # Output of softmax layer.\n",
            "    \n",
            "    \n",
            "    return o1, o2, probabilities\n",
            "\n",
            "\n",
            "o1, o2, predictions = neural_network(w1, w2, w3, b1, b2, b3)\n",
            "print(predictions)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Results of test\n",
            "\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
