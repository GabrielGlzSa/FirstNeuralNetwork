{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# My first Neural Network in Python\n",
            "\n",
            "\n",
            "In this Lab, you will program your first neural network using python and the numpy library for array operations. The difficulty will be increasing until the last exercise, where you will use all the functions you implemented to initialize your first neural network. In following labs, you will learn how to train a neural network.\n",
            "\n",
            "\n",
            "\n",
            "**IMPORTANT: This notebook will be graded every time you save the notebook. After saving the notebook, wait 1 second before refreshing the window to see the feedback appended at the end of the notebook. Do not modify any of the functions names that will be graded.** \n",
            "\n",
            "## Tasks\n",
            "\n",
            "- Implement the ReLu activation function.\n",
            "- Implement a softmax layer.\n",
            "- Initialize a 3 layer neural network.\n",
            "- Implement forward propagation through a neural network."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import numpy as np\n",
            "import math"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 1: Implement the ReLu activation function\n",
            "\n",
            "The Rectifier Linear (ReLU) is a function used as an activation function for neurons for its non-linearity. The function is applied per each neuron to calculate its output. The following figure shows that ReLu returns a 0 for negative values of x.\n",
            "\n",
            "\n",
            "![ReLu activation function](./images/relu.png)\n",
            "\n",
            "The formula for the ReLu function is given by the following Equation:\n",
            "\n",
            "$ReLu(x) = max(0,x)$\n",
            "\n",
            "\n",
            "ReLu exercises:\n",
            "\n",
            "- Exercise1: Implement the relu function for single value.\n",
            "- Exercise2: Implement the relu function for $k$ samples and $n$ neurons.\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 1\n",
            "def relu_function(x):\n",
            "    \"\"\"\n",
            "    Implement the ReLu function for a scalar.\n",
            "    x: a float\n",
            "    return: a float\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = 0 \n",
            "    return o\n",
            "\n",
            "\n",
            "output = relu_function(5)\n",
            "print(output) # Expected output: 5\n",
            "output = relu_function(-2)\n",
            "print(output) # Expected output: 0"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 2\n",
            "def relu_function_layer(x):\n",
            "    \"\"\"\n",
            "    Implement the ReLu function for an array.\n",
            "    HINT: there are multiple ways to achieve it using numpy.\n",
            "    x: a numpy array of floats\n",
            "    return: a numpy array of floats\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = np.maximum(x,0) \n",
            "    return o\n",
            "\n",
            "x = np.array([[5, -2, 3, 4, -5], [-1, 2, 3, 4, 5]])\n",
            "output = relu_function_layer(x)\n",
            "print(output) # Expected output: [[5, 0, 3, 4, 0], [0, 2, 3, 4, 5]]"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 2: Implement a softmax layer.\n",
            "\n",
            "The softmax layer is a dense layer that applies the softmax function to convert the logits into probabilities. The logits are the raw predictions of the neurons. The prediction of neuron $i$ is calculated using the following equation:\n",
            "\n",
            ">$z_i=\\mathbf{x} \\cdot \\mathbf{w}_i + b_i$,\n",
            "\n",
            "where $\\mathbf{x}$ is the input vector of the layer, $\\mathbf{w}$ is the weights of the neuron and $b$ is a scalar known as the bias. In contrast with the ReLu activation function that is applied to the output of only one neuron ReLu($\\mathbf{x} \\cdot \\mathbf{w}_i + b_i$), the softmax function takes into consideration the outputs of other layer to assign a probability to each neuron using the following equation:\n",
            "\n",
            ">$\\sigma(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{N} e^{z_j}}$\n",
            "\n",
            "where $z_i$ is the logit of layer $i$, $z_j$ is the logit of layer $j$ and $e$ is euler's number. Since the softmax function converts all logits into probabilities, the sum of the output of all neurons $n$ add to one: \n",
            "\n",
            "> $\\sum_{j}^{n} \\sigma(z_{j}) = 1$.\n",
            "\n",
            "\n",
            "In the following exercise, you will calculate the output of each neuron independently to understand how a neuron works. Then, you will calculate the logit for each neuron. Finally, you will use the numpy library to implement the softmax for all neurons in parallel.\n",
            "\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 3\n",
            "def calculate_neuron_logit(x, w, b):\n",
            "    \"\"\"\n",
            "    Calculate the logit of a neuron for a single sample by multiplying two vectors and summing the bias.\n",
            "    x: input vector\n",
            "    w: weight vector\n",
            "    b: floating-point number bias\n",
            "    return: a floating-point number.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    logit = np.dot(x,w)\n",
            "    return logit\n",
            "\n",
            "# Generate a dummy input with 10 features.\n",
            "x = np.random.rand(10)\n",
            "# Randomly initialize the weight for 10 features.\n",
            "w = np.random.rand(10)\n",
            "# Randomly initialize the bias.\n",
            "b = np.random.rand(1)\n",
            "\n",
            "logit = calculate_neuron_logit(x, w, b)\n",
            "print(logit)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Now that you have calculated the logit of a single neuron, it is time to process all neurons in parallel. A neural network process data in batches, meaning that $k$ samples are processed to the network at the same time. If the samples has $f$ features, the input is really a matrix with $k$ rows and $f$ columns. Since the input is a matrix, we shall refer to it as $\\mathbf{X}$. We can get the output of a neuron by multiplying $\\mathbf{X}$ by its weight vector $\\mathbf{w}$. \n",
            "\n",
            "Nonetheless, there is a better approach. Imagine we have a weight matrix $\\mathbf{W}$ that has all the weights of all neurons in a layer. One dimension would be the number of neurons $n$ and the other would be the number of input features $f$. We could calculate the output of all neurons for all samples with the dot product:\n",
            "\n",
            "$\\mathbf{z} = \\mathbf{X}\\cdot\\mathbf{W} + \\mathbf{b}$\n",
            "\n",
            "Since the dot product requires that the number of columns in $\\mathbf{X}$ be equal to the number of rows in $\\mathbf{W}$, the matrix $\\mathbf{W}$ must have $f$ rows and $n$ columns. In other words, all weights associated with neuron $i$ will be in column $i$. \n",
            "\n",
            "  "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 4\n",
            "def calculate_logits_layer(x, w, b):\n",
            "    \"\"\"\n",
            "    Calculate the logits of all neurons in parallel.\n",
            "    HINT: USE MATRIX MULTIPLICATION IN NUMPY.\n",
            "    x: input matrix\n",
            "    w: weight matrix\n",
            "    b: bias vector\n",
            "    return: a numpy array of logits.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    o = 0\n",
            "    return o\n",
            "\n",
            "# Generate a dummy input with 3 samples and 8 features.\n",
            "X = np.random.uniform(low=-1.0, high=1.0, size=(3,8))\n",
            "# Randomly initialize the weight for 8 features and 4 neurons.\n",
            "W = np.random.uniform(low=-1.0, high=1.0, size=(8, 4))\n",
            "# Randomly initialize the bias for 4 neurons.\n",
            "B = np.random.uniform(low=-1.0, high=1.0, size=4)\n",
            "\n",
            "logits = calculate_logits_layer(X, W, B)\n",
            "print(logits.shape) # Expected shape is (3, 4)\n",
            "print(logits)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 5\n",
            "def softmax_layer(x, w, b):\n",
            "    \"\"\"\n",
            "    Calculate the probability for each neuron using the softmax function.\n",
            "    HINT: You can use exp() available in numpy.\n",
            "    x: input matrix\n",
            "    w: weight matrix\n",
            "    b: bias vector\n",
            "    \"\"\"\n",
            "    logits = calculate_logits_layer(x, w, b)\n",
            "\n",
            "    probabilities = logits # Your code here\n",
            "    \n",
            "    \n",
            "    return probabilities\n",
            "\n",
            "\n",
            "probabilities = softmax_layer(X, W, B)\n",
            "print(probabilities)\n",
            "\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Task 3: Implement a neural network\n",
            "\n",
            "For this task, you will create a neural network using the functions you have implemented so far. The neural network must have the following architecture:\n",
            "\n",
            "1. Dense layer with ReLu activation function with 32 neurons.\n",
            "2. Dense layer with Relu activation function with 32 neurons.\n",
            "3. Dense layer with softmax activation function with 4 neurons.\n",
            "\n",
            "The weights of the neural network should be randomly initialized using a normal distribution. Take into consideration that the number of features per sample is 8."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Exercise 6\n",
            "def weight_initialization(f, n1, n2, n3):\n",
            "    \"\"\"\n",
            "    Randomly initialize the weights of a neural network \n",
            "    with 3 layers using a normal distribution.\n",
            "    HINT: Use numpy to generate random matrices.\n",
            "    f: number of features\n",
            "    n1: number of neurons in the first hidden layer\n",
            "    n2: number of neurons in the second hidden layer\n",
            "    n3: number of neurons in the output layer\n",
            "    return: a tuple of weight matrices\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "    w1 = np.random.normal(size=(10,8))\n",
            "    w2 = np.random.normal(size=(10,8))\n",
            "    w3 = 0\n",
            "    b1 = 0\n",
            "    b2 = 0\n",
            "    b3 = 0\n",
            "    return w1, w2, w3, b1, b2, b3\n",
            "\n",
            "# Example of weight initialization for layers with 10, 8, 6 and 4 neurons, respectively.\n",
            "w1, w2, w3, b1, b2, b3 = weight_initialization(10, 8, 6, 4)\n",
            "print(w1.shape) # Expected output: (10, 8)\n",
            "print(w2.shape) # Expected output: (8, 6)\n",
            "print(w3.shape) # Expected output: (6, 4)\n",
            "print(b1.shape) # Expected output: (8,)\n",
            "print(b2.shape) # Expected output: (6,)\n",
            "print(b3.shape) # Expected output: (4,)\n",
            "\n",
            "# Modify the parameters to create the specified neural network architecture.\n",
            "w1, w2, w3, b1, b2, b3 = weight_initialization(10, 8, 6, 4)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The process of calculating the output of a neural network is called by several names like fordward propagation and inference. In the following exercise, you will use the functions you implemented for forward propagation. "
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def neural_network(x, w1, b1, w2, b2, w3, b3):\n",
            "    \"\"\"\n",
            "    Use the functions you implemented to create a neural network with 3 layers and predict the output for x.\n",
            "    x: input matrix\n",
            "    wn: weight matrix of layer n.\n",
            "    bn: bias vector of layer n.\n",
            "    return: tuple of numpy arrays for output of first layer, output of second layer, output of the network.\n",
            "    \"\"\"\n",
            "    # Your code here\n",
            "\n",
            "    o1 = 0\n",
            "    o2 = 0\n",
            "    probabilities = 0\n",
            "    \n",
            "    \n",
            "    return probabilities\n",
            "\n",
            "\n",
            "o1, o2, predictions = neural_network(w1, w2, w3, b1, b2, b3)\n",
            "print(predictions)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Results of test\n",
            "\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "Python 3 (ipykernel)",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.11"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 4
}
